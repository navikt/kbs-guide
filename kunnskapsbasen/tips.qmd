# Sammenligning av vektordatabaser {#sec-tips-testing}

For å vurdere vektordatabasen må man opprette et set med spørsmål og
artikkelsvar fra kunnskapsbasen og sammenligne hvor godt hver database gjør det.
Dette er dog tidkrevende og kan være vanskelig å gjøre uten ekspertkompetanse på
den originale kunnskapsbasen.

## Syntetiske spørsmål

For å komme rundt begrensningen med ekspertkunnskap på kunnskapsbasen kan man
prøve å benytte en språkmodell til å lage spørsmål til kunnskapsbasen. Dette kan
være så enkelt som å be språkmodellen lage et spørsmål som bare kan besvares av
teksten som kommer sammen med spørsmålet. En bakdel med dette er jo at man ikke
nødvendigvis kan si om spørsmålene er gode uten ekspertkunnskap, men samtidig så
vil det være identiske spørsmål som må besvares, så alle vektordatabasene burde
ha lik mulighet til å gjøre det bra eller dårlig.

I vårt arbeid med NKS Digital Assistent har vi gjort en sammenligning basert på
syntetiske spørsmål.

- [Fremgangsmåte for å generere spørsmål (ekstern
datafortelling)](https://data.ansatt.nav.no/story/98cec0de-b872-4713-a56f-9bdb49112345/index.html)

## Evalueringsmetrikker

Med de syntetiske spørsmålene har vi ønsket å sammenligne hvor gode de forskjellige vektordatabasene er på å finne tilbake "fasiten", det vil si teksten som spørsmålet ble generert fra. 

To metrikker som ofte går igjen i evaluering av RAG-systemer er `hit rate` og `mean reciprocal rank`. Se f.eks [LlamaIndex](https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83). 

### Hit rate
For hver testcase sjekkes rett og slett om den korrekte teksten er blant de topp `k` returnerte resultatene. Den samlede hitraten tilsvarer da andelen caser der korrekt tekst ble funnet.  

### Mean reciprocal rank (MRR)
For hver testcase finnes først ranken til den høyest scorede korrekte teksten. Reciprocal rank er definert som $\frac{1}{rank}$, så dersom korrekt artikkel er rangert som nummer 1 vil reciprocal rank være 1. Dersom korrekt artikkel er rangert som nummer 2 vil reciprocal rank være $\frac{1}{2}$ osv. Den samlede MRR-scoren er gjennomsnittet av disse rank-scorene per case. 

### Fordeler og ulemper
Fordelen med `hit rate` og `MRR` er at de begge de begge er enkle å forstå og implementere. 

Ulempen er kanskje at de er litt for enkle:  

- Vi klassifiserer i praksis de returnerte dokumentene binært som "riktig" eller "feil". I virkeligheten vil dokumentene ha ulike grad av relevans, der tekster som ikke er "fasit"-teksten likevel kan handle om samme tema som det syntetiske spørsmålet. Her kan man vurdere å utarbeide en mer avansert relevans-scoring og anvende evalueringsmetrikker som hensyntar dette.   
- `hit rate` og `MRR` vil egne seg  mindre bra til å evaluere caser der det er meningen å returnere mer enn 1 tekst. Selv om den type caser vil være viktig for NKS Digital assistent å beherske, så har vi i sammenligningen av vektordatabasene holdt dette utenfor.  
- `hit rate` og `MRR` gir heller ingen "straff" til vektordatabasene dersom de returnerer irrelevante dokumenter (så lenge riktig dokument blir funnet.)  

En annen faktor å ha med i betraktingen er at noen av tekstene som har vært kilde for et syntetisk spørsmål, har i vektordatabasene blitt splittet opp i flere chunks. Når vi ber om å få returnert de topp `k` mest relevante dokumentene for et spørsmål kan det derfor hende at samme tekst vil bli representert mer enn én gang i resultatsettet. Vi har valgt å holde evalueringen på "tekst-nivå" ved å gi uttelling til vektordatabasen for den høyest rangerte chunken fra riktige tekst. 

- [Sammenligning av forskjellige vektordatabaser på NKS sin kunnskapsbase
(ekstern
datafortelling)](https://data.ansatt.nav.no/story/e17a7b82-8721-4357-ba92-4c734f68e9f5/index.html#resultater)